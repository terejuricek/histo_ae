{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74139fe",
   "metadata": {},
   "source": [
    "# Autoencoder for Histopathology WSI Analysis\n",
    "### End-to-End Blueprint for Dimensionality Reduction\n",
    "\n",
    "*A practical, comprehensive guide for designing an autoencoder (AE) to learn compact, high-quality embeddings from whole-slide images (WSIs) for dimensionality reduction and downstream analysis.*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Define the Objective Precisely\n",
    "\n",
    "### Primary Goal\n",
    "Learn tile-level embeddings that capture tissue morphology and stain characteristics while discarding background and slide-specific artifacts.\n",
    "\n",
    "### Downstream Applications\n",
    "Clarify whether you need embeddings for:\n",
    "- **(a)** Slide-level modeling (e.g., MIL for diagnosis/prognosis)\n",
    "- **(b)** Content-based retrieval\n",
    "- **(c)** Clustering/tile curation  \n",
    "- **(d)** Input to classical DR (PCA/UMAP) for visualization\n",
    "\n",
    "> **Note:** This choice affects architecture, loss weighting, and aggregation strategies.\n",
    "\n",
    "### Scale Considerations\n",
    "- Decide magnification(s) relevant to your task\n",
    "- **5×**: Tissue layout and architecture\n",
    "- **10×–20×**: Cellular detail and morphology\n",
    "- **Multi-scale**: Design variant from the start if you need both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c96105f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules and libraries\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ExifTags\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a1adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING UP CONFIGURATION...\n",
      "Configuration set:\n",
      " - Input size: 150x150\n",
      " - Latent dim: 256\n",
      " - Channels: 3\n",
      " - Output dir: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits\n",
      " - Data splits: 70.0% train, 15.0% val, 15.0% test\n"
     ]
    }
   ],
   "source": [
    "# ESSENTIAL CONFIGURATION - Required Constants\n",
    "print(\"SETTING UP CONFIGURATION...\")\n",
    "\n",
    "# Model Configuration (based on patch analysis)\n",
    "INPUT_SIZE = 150  # Patch size from analysis: 150x150 pixels\n",
    "LATENT_DIM = 256  # Latent space dimensionality\n",
    "NUM_CHANNELS = 3  # RGB channels\n",
    "\n",
    "# Training Configuration\n",
    "RANDOM_SEED = 42\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15  \n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Data Paths\n",
    "OUTPUT_BASE_DIR = \"/Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits\"\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\" - Input size: {INPUT_SIZE}x{INPUT_SIZE}\")\n",
    "print(f\" - Latent dim: {LATENT_DIM}\")\n",
    "print(f\" - Channels: {NUM_CHANNELS}\")\n",
    "print(f\" - Output dir: {OUTPUT_BASE_DIR}\")\n",
    "print(f\" - Data splits: {TRAIN_RATIO:.1%} train, {VAL_RATIO:.1%} val, {TEST_RATIO:.1%} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8deaac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: parse error near `-m'\n"
     ]
    }
   ],
   "source": [
    "# analyze provided patches\n",
    "PATCHES_PATH = \"/Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/01_TUMOR\"\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fd619cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive patch analysis...\n",
      "Analyzing patches in: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/01_TUMOR\n",
      "Found 0 image files\n",
      "No image files found! Check the path.\n",
      "Directory exists: False\n",
      "\n",
      "Patch analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Patch Analysis Script\n",
    "print(\"Starting comprehensive patch analysis...\")\n",
    "print(f\"Analyzing patches in: {PATCHES_PATH}\")\n",
    "\n",
    "# 1. DISCOVERY: Find all image files\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.tiff', '*.tif', '*.bmp']\n",
    "all_patches = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    patches = glob.glob(os.path.join(PATCHES_PATH, '**', ext), recursive=True)\n",
    "    patches.extend(glob.glob(os.path.join(PATCHES_PATH, '**', ext.upper()), recursive=True))\n",
    "    all_patches.extend(patches)\n",
    "\n",
    "print(f\"Found {len(all_patches)} image files\")\n",
    "\n",
    "if len(all_patches) == 0:\n",
    "    print(\"No image files found! Check the path.\")\n",
    "    print(f\"Directory exists: {os.path.exists(PATCHES_PATH)}\")\n",
    "    if os.path.exists(PATCHES_PATH):\n",
    "        print(f\"Contents: {os.listdir(PATCHES_PATH)}\")\n",
    "else:\n",
    "    # 2. BASIC STATISTICS\n",
    "    file_extensions = Counter([Path(p).suffix.lower() for p in all_patches])\n",
    "    print(f\"\\nFile Extensions Found:\")\n",
    "    for ext, count in file_extensions.items():\n",
    "        print(f\"  {ext}: {count:,} files\")\n",
    "    \n",
    "    # 3. DETAILED ANALYSIS (sample subset for speed)\n",
    "    sample_size = min(1000, len(all_patches))  # Analyze max 1000 for speed\n",
    "    sample_patches = np.random.choice(all_patches, sample_size, replace=False)\n",
    "    \n",
    "    print(f\"\\nAnalyzing {sample_size} sample patches...\")\n",
    "    \n",
    "    # Initialize data collectors\n",
    "    patch_data = []\n",
    "    errors = []\n",
    "    \n",
    "    for i, patch_path in enumerate(tqdm(sample_patches, desc=\"Analyzing patches\")):\n",
    "        try:\n",
    "            with Image.open(patch_path) as img:\n",
    "                # Basic image properties\n",
    "                width, height = img.size\n",
    "                mode = img.mode\n",
    "                format_type = img.format\n",
    "                \n",
    "                # File size\n",
    "                file_size = os.path.getsize(patch_path) / 1024  # KB\n",
    "                \n",
    "                # Color analysis (for RGB images)\n",
    "                if mode in ['RGB', 'RGBA']:\n",
    "                    img_array = np.array(img.convert('RGB'))\n",
    "                    \n",
    "                    # Basic statistics\n",
    "                    mean_intensity = np.mean(img_array)\n",
    "                    std_intensity = np.std(img_array)\n",
    "                    \n",
    "                    # Channel-wise analysis\n",
    "                    r_mean, g_mean, b_mean = np.mean(img_array, axis=(0,1))\n",
    "                    \n",
    "                    # Simple tissue detection (non-white pixels)\n",
    "                    gray = np.mean(img_array, axis=2)\n",
    "                    tissue_ratio = np.mean(gray < 200)  # Pixels that are not close to white\n",
    "                    \n",
    "                    # Blur detection (Laplacian variance)\n",
    "                    from scipy import ndimage\n",
    "                    blur_score = ndimage.laplace(gray).var()\n",
    "                    \n",
    "                else:\n",
    "                    mean_intensity = std_intensity = 0\n",
    "                    r_mean = g_mean = b_mean = 0\n",
    "                    tissue_ratio = 0\n",
    "                    blur_score = 0\n",
    "                \n",
    "                # Try to get DPI/resolution info\n",
    "                dpi = img.info.get('dpi', (None, None))\n",
    "                \n",
    "                patch_data.append({\n",
    "                    'filename': Path(patch_path).name,\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'mode': mode,\n",
    "                    'format': format_type,\n",
    "                    'file_size_kb': file_size,\n",
    "                    'dpi_x': dpi[0] if dpi[0] else None,\n",
    "                    'dpi_y': dpi[1] if dpi[1] else None,\n",
    "                    'mean_intensity': mean_intensity,\n",
    "                    'std_intensity': std_intensity,\n",
    "                    'r_mean': r_mean,\n",
    "                    'g_mean': g_mean,\n",
    "                    'b_mean': b_mean,\n",
    "                    'tissue_ratio': tissue_ratio,\n",
    "                    'blur_score': blur_score\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors.append({'file': patch_path, 'error': str(e)})\n",
    "    \n",
    "    # 4. ANALYSIS RESULTS\n",
    "    df = pd.DataFrame(patch_data)\n",
    "    \n",
    "    print(f\"\\nANALYSIS RESULTS:\")\n",
    "    print(f\"Successfully analyzed: {len(df)} patches\")\n",
    "    print(f\"Errors encountered: {len(errors)}\")\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        print(f\"\\nDIMENSIONS:\")\n",
    "        print(f\"Width range: {df['width'].min()} - {df['width'].max()} pixels\")\n",
    "        print(f\"Height range: {df['height'].min()} - {df['height'].max()} pixels\")\n",
    "        print(f\"Most common size: {df.groupby(['width', 'height']).size().idxmax()}\")\n",
    "        print(f\"Unique sizes found: {len(df.groupby(['width', 'height']))}\")\n",
    "        \n",
    "        print(f\"\\nCOLOR PROPERTIES:\")\n",
    "        print(f\"Color modes: {df['mode'].value_counts().to_dict()}\")\n",
    "        print(f\"File formats: {df['format'].value_counts().to_dict()}\")\n",
    "        print(f\"Mean intensity: {df['mean_intensity'].mean():.1f} ± {df['mean_intensity'].std():.1f}\")\n",
    "        \n",
    "        print(f\"\\nFILE PROPERTIES:\")\n",
    "        print(f\"File size range: {df['file_size_kb'].min():.1f} - {df['file_size_kb'].max():.1f} KB\")\n",
    "        print(f\"Average file size: {df['file_size_kb'].mean():.1f} KB\")\n",
    "        \n",
    "        # DPI analysis\n",
    "        if df['dpi_x'].notna().any():\n",
    "            print(f\"\\nRESOLUTION:\")\n",
    "            unique_dpis = df[['dpi_x', 'dpi_y']].drop_duplicates()\n",
    "            print(f\"DPI information available for {df['dpi_x'].notna().sum()} files\")\n",
    "            print(f\"Unique DPI values: {len(unique_dpis)}\")\n",
    "            if len(unique_dpis) <= 5:\n",
    "                for _, row in unique_dpis.iterrows():\n",
    "                    print(f\"  {row['dpi_x']} x {row['dpi_y']} DPI\")\n",
    "        else:\n",
    "            print(f\"\\nRESOLUTION: No DPI information found in image metadata\")\n",
    "        \n",
    "        print(f\"\\nTISSUE ANALYSIS:\")\n",
    "        print(f\"Average tissue ratio: {df['tissue_ratio'].mean():.2f} (1.0 = all tissue, 0.0 = all background)\")\n",
    "        print(f\"Tissue ratio range: {df['tissue_ratio'].min():.2f} - {df['tissue_ratio'].max():.2f}\")\n",
    "        \n",
    "        print(f\"\\nIMAGE QUALITY:\")\n",
    "        print(f\"Blur score range: {df['blur_score'].min():.1f} - {df['blur_score'].max():.1f}\")\n",
    "        print(f\"Average blur score: {df['blur_score'].mean():.1f} (higher = sharper)\")\n",
    "        \n",
    "        # 5. CONSISTENCY CHECK\n",
    "        print(f\"\\nCONSISTENCY CHECK:\")\n",
    "        \n",
    "        # Check if all patches have same dimensions\n",
    "        unique_sizes = df.groupby(['width', 'height']).size()\n",
    "        if len(unique_sizes) == 1:\n",
    "            w, h = unique_sizes.index[0]\n",
    "            print(f\"All patches have consistent size: {w}×{h} pixels\")\n",
    "        else:\n",
    "            print(f\"Found {len(unique_sizes)} different sizes:\")\n",
    "            for (w, h), count in unique_sizes.head().items():\n",
    "                print(f\"   {w}×{h}: {count} patches ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Check color mode consistency\n",
    "        if len(df['mode'].unique()) == 1:\n",
    "            print(f\"All patches have consistent color mode: {df['mode'].iloc[0]}\")\n",
    "        else:\n",
    "            print(f\"Mixed color modes found: {df['mode'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Check file format consistency\n",
    "        if len(df['format'].unique()) == 1:\n",
    "            print(f\"All patches have consistent format: {df['format'].iloc[0]}\")\n",
    "        else:\n",
    "            print(f\"Mixed formats found: {df['format'].value_counts().to_dict()}\")\n",
    "\n",
    "        # 6. RECOMMENDATIONS\n",
    "        print(f\"\\nRECOMMENDATIONS:\")\n",
    "        \n",
    "        most_common_size = df.groupby(['width', 'height']).size().idxmax()\n",
    "        w, h = most_common_size\n",
    "        print(f\"• Standard patch size to use: {w}×{h} pixels\")\n",
    "        \n",
    "        if df['tissue_ratio'].mean() < 0.5:\n",
    "            print(f\"• Consider filtering patches with tissue_ratio < 0.3 to remove background\")\n",
    "        \n",
    "        if df['blur_score'].std() > df['blur_score'].mean() * 0.5:\n",
    "            print(f\"• Consider filtering blurry patches (blur_score < {df['blur_score'].quantile(0.25):.0f})\")\n",
    "        \n",
    "        avg_size_kb = df['file_size_kb'].mean()\n",
    "        if avg_size_kb > 500:\n",
    "            print(f\"• Large file sizes ({avg_size_kb:.0f} KB avg) - consider compression for training\")\n",
    "        \n",
    "        # 7. VISUALIZATIONS\n",
    "        if len(df) > 10:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "            fig.suptitle('Patch Analysis Dashboard', fontsize=16)\n",
    "            \n",
    "            # Size distribution\n",
    "            size_counts = df.groupby(['width', 'height']).size().reset_index(name='count')\n",
    "            axes[0,0].bar(range(len(size_counts)), size_counts['count'])\n",
    "            axes[0,0].set_title('Size Distribution')\n",
    "            axes[0,0].set_xlabel('Size variants')\n",
    "            axes[0,0].set_ylabel('Count')\n",
    "            \n",
    "            # File size distribution\n",
    "            axes[0,1].hist(df['file_size_kb'], bins=30, alpha=0.7)\n",
    "            axes[0,1].set_title('File Size Distribution')\n",
    "            axes[0,1].set_xlabel('File Size (KB)')\n",
    "            axes[0,1].set_ylabel('Count')\n",
    "            \n",
    "            # Tissue ratio distribution\n",
    "            axes[0,2].hist(df['tissue_ratio'], bins=30, alpha=0.7)\n",
    "            axes[0,2].set_title('Tissue Ratio Distribution')\n",
    "            axes[0,2].set_xlabel('Tissue Ratio')\n",
    "            axes[0,2].set_ylabel('Count')\n",
    "            \n",
    "            # Color channel means\n",
    "            axes[1,0].hist([df['r_mean'], df['g_mean'], df['b_mean']], \n",
    "                          bins=30, alpha=0.5, label=['Red', 'Green', 'Blue'])\n",
    "            axes[1,0].set_title('Color Channel Distributions')\n",
    "            axes[1,0].set_xlabel('Mean Intensity')\n",
    "            axes[1,0].legend()\n",
    "            \n",
    "            # Blur score distribution\n",
    "            axes[1,1].hist(df['blur_score'], bins=30, alpha=0.7)\n",
    "            axes[1,1].set_title('Blur Score Distribution')\n",
    "            axes[1,1].set_xlabel('Blur Score (higher=sharper)')\n",
    "            axes[1,1].set_ylabel('Count')\n",
    "            \n",
    "            # Mean vs std intensity scatter\n",
    "            axes[1,2].scatter(df['mean_intensity'], df['std_intensity'], alpha=0.6)\n",
    "            axes[1,2].set_title('Intensity Mean vs Std')\n",
    "            axes[1,2].set_xlabel('Mean Intensity')\n",
    "            axes[1,2].set_ylabel('Std Intensity')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Store results for future use\n",
    "        globals()['patch_analysis_df'] = df\n",
    "        globals()['patch_analysis_summary'] = {\n",
    "            'total_patches': len(all_patches),\n",
    "            'analyzed_patches': len(df),\n",
    "            'most_common_size': most_common_size,\n",
    "            'consistent_size': len(unique_sizes) == 1,\n",
    "            'avg_tissue_ratio': df['tissue_ratio'].mean(),\n",
    "            'avg_file_size_kb': df['file_size_kb'].mean(),\n",
    "            'color_mode': df['mode'].iloc[0] if len(df['mode'].unique()) == 1 else 'mixed'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nResults stored in variables:\")\n",
    "        print(f\"• patch_analysis_df: Detailed DataFrame with all metrics\")\n",
    "        print(f\"• patch_analysis_summary: Summary statistics dictionary\")\n",
    "    \n",
    "    # Show errors if any\n",
    "    if errors:\n",
    "        print(f\"\\nERRORS ENCOUNTERED:\")\n",
    "        for error in errors[:5]:  # Show first 5 errors\n",
    "            print(f\"  {Path(error['file']).name}: {error['error']}\")\n",
    "        if len(errors) > 5:\n",
    "            print(f\"  ... and {len(errors)-5} more errors\")\n",
    "\n",
    "print(\"\\nPatch analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e2a08",
   "metadata": {},
   "source": [
    "## 2. WSI Reading and Tiling\n",
    "\n",
    "### Required Libraries\n",
    "- **OpenSlide**, **pyvips**, **tifffile**, or **cuCIM** for WSI pyramid reading\n",
    "\n",
    "### Tissue Masking Strategy\n",
    "1. Compute low-resolution tissue mask to exclude blank glass\n",
    "2. Use **HSV** or **LAB** thresholding + morphology operations\n",
    "3. **Discard tiles** with >80–90% background\n",
    "\n",
    "### Tile Configuration\n",
    "| Parameter | Recommended Value | Notes |\n",
    "|-----------|------------------|-------|\n",
    "| **Tile Size** | 256×256 or 512×512 pixels | At 10× or 20× magnification |\n",
    "| **Stride** | Equal to tile size | Non-overlapping tiles |\n",
    "| **Overlap** | 50% (optional) | Only if dataset is small |\n",
    "\n",
    "### Sampling Policy\n",
    "- **Between slides**: Sample uniformly across all slides\n",
    "- **Within slides**: Stratify by tissue mask to avoid background over-representation\n",
    "- **Special cases**: If region annotations exist, oversample rare/critical morphologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa876228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing patches: 100%|██████████| 625/625 [00:00<00:00, 35384.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIMENSIONS:\n",
      "Width range: 150 - 150 pixels\n",
      "Height range: 150 - 150 pixels\n",
      "Most common size: (np.int64(150), np.int64(150))\n",
      "Unique sizes found: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Size Analysis\n",
    "patch_data = []\n",
    "errors = []\n",
    "for i, patch_path in enumerate(tqdm(sample_patches, desc=\"Analyzing patches\")):\n",
    "    try:\n",
    "        with Image.open(patch_path) as img:\n",
    "            # Basic image properties\n",
    "            width, height = img.size\n",
    "            mode = img.mode\n",
    "            format_type = img.format\n",
    "            \n",
    "            # File size\n",
    "            file_size = os.path.getsize(patch_path) / 1024  # KB\n",
    "            \n",
    "            \n",
    "            patch_data.append({\n",
    "                'filename': Path(patch_path).name,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'mode': mode,\n",
    "                'format': format_type,\n",
    "                'file_size_kb': file_size,\n",
    "\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors.append({'file': patch_path, 'error': str(e)})\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nDIMENSIONS:\")\n",
    "    print(f\"Width range: {df['width'].min()} - {df['width'].max()} pixels\")\n",
    "    print(f\"Height range: {df['height'].min()} - {df['height'].max()} pixels\")\n",
    "    print(f\"Most common size: {df.groupby(['width', 'height']).size().idxmax()}\")\n",
    "    print(f\"Unique sizes found: {len(df.groupby(['width', 'height']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd73f72c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/Users/terezajurickova/Documents/GitHub/histo_ae/data/test_img_1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m io, color, filters, feature\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m img = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/terezajurickova/Documents/GitHub/histo_ae/data/test_img_1.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[32m      7\u001b[39m gray = color.rgb2gray(img)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Apply different feature extractors\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Sobel edge detection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/skimage/_shared/utils.py:328\u001b[39m, in \u001b[36mdeprecate_parameter.__call__.<locals>.fixed_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.new_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# Assign old value to new one\u001b[39;00m\n\u001b[32m    326\u001b[39m         kwargs[\u001b[38;5;28mself\u001b[39m.new_name] = deprecated_value\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/skimage/io/_io.py:82\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(fname, as_gray, plugin, **plugin_args)\u001b[39m\n\u001b[32m     79\u001b[39m         plugin = \u001b[33m'\u001b[39m\u001b[33mtifffile\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname, _hide_plugin_deprecation_warnings():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     img = \u001b[43mcall_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimread\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mplugin_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[33m'\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/skimage/_shared/utils.py:538\u001b[39m, in \u001b[36mdeprecate_func.__call__.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    536\u001b[39m stacklevel = \u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.get_stack_length(func) - stack_rank\n\u001b[32m    537\u001b[39m warnings.warn(message, category=\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=stacklevel)\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/skimage/io/manage_plugins.py:254\u001b[39m, in \u001b[36mcall_plugin\u001b[39m\u001b[34m(kind, *args, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[32m    252\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCould not find the plugin \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/skimage/io/_plugins/imageio_plugin.py:11\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimread\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     out = np.asarray(\u001b[43mimageio_imread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out.flags[\u001b[33m'\u001b[39m\u001b[33mWRITEABLE\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     13\u001b[39m         out = out.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imageio/v3.py:53\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m] = index\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mplugin_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(img_file.read(**call_kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imageio/core/imopen.py:113\u001b[39m, in \u001b[36mimopen\u001b[39m\u001b[34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m     request.format_hint = format_hint\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     request = \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m source = \u001b[33m\"\u001b[39m\u001b[33m<bytes>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imageio/core/request.py:248\u001b[39m, in \u001b[36mRequest.__init__\u001b[39m\u001b[34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imageio/core/request.py:408\u001b[39m, in \u001b[36mRequest._parse_uri\u001b[39m\u001b[34m(self, uri)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(fn):\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % fn)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    410\u001b[39m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[32m    411\u001b[39m     dn = os.path.dirname(fn)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file: '/Users/terezajurickova/Documents/GitHub/histo_ae/data/test_img_1.jpg'"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, color, filters, feature\n",
    "\n",
    "# Load image\n",
    "img = io.imread(\"/Users/terezajurickova/Documents/GitHub/histo_ae/data/test_img_1.jpg\")   \n",
    "gray = color.rgb2gray(img)\n",
    "\n",
    "# Apply different feature extractors\n",
    "# Sobel edge detection\n",
    "edges_sobel = filters.sobel(gray)\n",
    "\n",
    "# Canny edge detection\n",
    "edges_canny = feature.canny(gray, sigma=2)\n",
    "\n",
    "# HOG (Histogram of Oriented Gradients)\n",
    "hog_features, hog_image = feature.hog(\n",
    "    gray, pixels_per_cell=(16, 16),\n",
    "    cells_per_block=(2, 2),\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# 3. Show results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 8))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(gray, cmap='gray')\n",
    "ax[0].set_title(\"Grayscale Image\")\n",
    "\n",
    "ax[1].imshow(edges_sobel, cmap='gray')\n",
    "ax[1].set_title(\"Sobel Edges\")\n",
    "\n",
    "ax[2].imshow(edges_canny, cmap='gray')\n",
    "ax[2].set_title(\"Canny Edges\")\n",
    "\n",
    "ax[3].imshow(hog_image, cmap='gray')\n",
    "ax[3].set_title(\"HOG Features\")\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3cbc0",
   "metadata": {},
   "source": [
    "## 3. Color Handling and Stain Normalization\n",
    "\n",
    "### Strategy Options\n",
    "\n",
    "#### Option A: Normalize\n",
    "- Apply **Macenko** or **Vahadane** normalization\n",
    "- Reduces inter-slide stain variability\n",
    "- More consistent but potentially less generalizable\n",
    "\n",
    "#### Option B: Augment  \n",
    "- Leave raw stains unchanged\n",
    "- Apply strong, histology-aware color jitter as augmentation\n",
    "- Adjust hematoxylin/eosin channel intensities, brightness/contrast/gamma\n",
    "- Often generalizes better than hard normalization alone\n",
    "\n",
    "### Recommended Approach\n",
    "> **For multi-lab/scanner datasets**: Light normalization + moderate color augmentation\n",
    "\n",
    "This hybrid approach balances consistency with robustness to unseen stain variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb652d80",
   "metadata": {},
   "source": [
    "## 4. Data Splits (Avoid Leakage)\n",
    "\n",
    "### CRITICAL: Patient-Level Splits\n",
    "> **Always split by patient or case, never by tiles**\n",
    "\n",
    "This prevents optimistic validation scores from data leakage.\n",
    "\n",
    "### Stratification Strategy\n",
    "- Keep **lab/scanner/source** balanced across splits when possible\n",
    "- Maintain similar distributions of:\n",
    "  - Tissue types\n",
    "  - Staining protocols  \n",
    "  - Scanner characteristics\n",
    "\n",
    "### Typical Split Ratios\n",
    "- **Training**: 70-80%\n",
    "- **Validation**: 10-15% \n",
    "- **Testing**: 10-15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ae1b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data splitting process...\n",
      "Total patches found: 625\n",
      "Created directory: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits/train\n",
      "Created directory: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits/val\n",
      "Created directory: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits/test\n",
      "Prepared 625 patch files for splitting\n",
      "\n",
      "Split Statistics:\n",
      "Train: 437 patches (69.9%)\n",
      "Validation: 94 patches (15.0%)\n",
      "Test: 94 patches (15.0%)\n",
      "Total: 625 patches\n",
      "\n",
      "Ready to copy files to: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits\n",
      "This operation will copy all patches to separate train/val/test directories.\n",
      "Creating split manifests (without copying files yet)...\n",
      "Created manifest: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits/train_manifest.txt\n",
      "Created manifest: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits/val_manifest.txt\n",
      "Created manifest: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits/test_manifest.txt\n",
      "\n",
      "Quality Checks:\n",
      "Train ∩ Val: 0 (should be 0)\n",
      "Train ∩ Test: 0 (should be 0)\n",
      "Val ∩ Test: 0 (should be 0)\n",
      "Total unique files: 625\n",
      "\n",
      "Split Quality Analysis (based on sample analysis):\n",
      "\n",
      "TRAIN Split Analysis (437 analyzed files):\n",
      "  Avg tissue ratio: 0.944\n",
      "  Avg file size: 66.8 KB\n",
      "  Avg blur score: 541.0\n",
      "\n",
      "VAL Split Analysis (94 analyzed files):\n",
      "  Avg tissue ratio: 0.946\n",
      "  Avg file size: 66.8 KB\n",
      "  Avg blur score: 527.9\n",
      "\n",
      "TEST Split Analysis (94 analyzed files):\n",
      "  Avg tissue ratio: 0.945\n",
      "  Avg file size: 66.8 KB\n",
      "  Avg blur score: 559.0\n",
      "\n",
      "============================================================\n",
      "SPLIT PREPARATION COMPLETE!\n",
      "Manifests created in: /Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits\n",
      "\n",
      "To actually copy the files, run:\n",
      "copy_train, err_train = copy_files_to_split(train_files, 'train')\n",
      "copy_val, err_val = copy_files_to_split(val_files, 'val')\n",
      "copy_test, err_test = copy_files_to_split(test_files, 'test')\n",
      "\n",
      "Split information stored in 'data_splits' variable for future use.\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting Script for Patch Dataset\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Starting data splitting process...\")\n",
    "print(f\"Total patches found: {len(all_patches):,}\")\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_BASE_DIR = \"/Volumes/tereju_disk/RECETOX/autoencoder/coad/kather01/splits\"\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create output directories\n",
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    split_dir = os.path.join(OUTPUT_BASE_DIR, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    print(f\"Created directory: {split_dir}\")\n",
    "\n",
    "# Prepare patch list for splitting\n",
    "patch_files = [{'path': path, 'filename': Path(path).name} for path in all_patches]\n",
    "print(f\"Prepared {len(patch_files)} patch files for splitting\")\n",
    "\n",
    "# Perform stratified splitting (first split: train vs temp)\n",
    "train_files, temp_files = train_test_split(\n",
    "    patch_files, \n",
    "    test_size=(VAL_RATIO + TEST_RATIO),\n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: temp into val and test\n",
    "val_files, test_files = train_test_split(\n",
    "    temp_files,\n",
    "    test_size=TEST_RATIO/(VAL_RATIO + TEST_RATIO),\n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Print split statistics\n",
    "print(f\"\\nSplit Statistics:\")\n",
    "print(f\"Train: {len(train_files):,} patches ({len(train_files)/len(patch_files)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_files):,} patches ({len(val_files)/len(patch_files)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_files):,} patches ({len(test_files)/len(patch_files)*100:.1f}%)\")\n",
    "print(f\"Total: {len(train_files) + len(val_files) + len(test_files):,} patches\")\n",
    "\n",
    "# Function to copy files to destination\n",
    "def copy_files_to_split(file_list, split_name, copy_files=True):\n",
    "    \"\"\"Copy files to the appropriate split directory\"\"\"\n",
    "    split_dir = os.path.join(OUTPUT_BASE_DIR, split_name)\n",
    "    \n",
    "    copied_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    print(f\"\\nCopying {len(file_list)} files to {split_name} directory...\")\n",
    "    \n",
    "    for file_info in tqdm(file_list, desc=f\"Copying {split_name}\"):\n",
    "        source_path = file_info['path']\n",
    "        dest_path = os.path.join(split_dir, file_info['filename'])\n",
    "        \n",
    "        try:\n",
    "            if copy_files:\n",
    "                shutil.copy2(source_path, dest_path)\n",
    "            copied_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {file_info['filename']}: {e}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    print(f\"Successfully copied {copied_count} files to {split_name}\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors encountered: {error_count}\")\n",
    "    \n",
    "    return copied_count, error_count\n",
    "\n",
    "# Ask user if they want to actually copy files (since this can take time)\n",
    "print(f\"\\nReady to copy files to: {OUTPUT_BASE_DIR}\")\n",
    "print(\"This operation will copy all patches to separate train/val/test directories.\")\n",
    "\n",
    "# For safety, let's create a manifest first without copying\n",
    "print(\"Creating split manifests (without copying files yet)...\")\n",
    "\n",
    "# Create manifests for each split\n",
    "manifests = {\n",
    "    'train': train_files,\n",
    "    'val': val_files, \n",
    "    'test': test_files\n",
    "}\n",
    "\n",
    "for split_name, file_list in manifests.items():\n",
    "    manifest_path = os.path.join(OUTPUT_BASE_DIR, f\"{split_name}_manifest.txt\")\n",
    "    \n",
    "    with open(manifest_path, 'w') as f:\n",
    "        f.write(f\"# {split_name.upper()} SET MANIFEST\\n\")\n",
    "        f.write(f\"# Generated on: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"# Total files: {len(file_list)}\\n\")\n",
    "        f.write(f\"# Random seed: {RANDOM_SEED}\\n\\n\")\n",
    "        \n",
    "        for file_info in file_list:\n",
    "            f.write(f\"{file_info['path']}\\n\")\n",
    "    \n",
    "    print(f\"Created manifest: {manifest_path}\")\n",
    "\n",
    "# Quality check: verify no overlaps between splits\n",
    "train_files_set = set(f['filename'] for f in train_files)\n",
    "val_files_set = set(f['filename'] for f in val_files)\n",
    "test_files_set = set(f['filename'] for f in test_files)\n",
    "\n",
    "print(f\"\\nQuality Checks:\")\n",
    "print(f\"Train ∩ Val: {len(train_files_set & val_files_set)} (should be 0)\")\n",
    "print(f\"Train ∩ Test: {len(train_files_set & test_files_set)} (should be 0)\")\n",
    "print(f\"Val ∩ Test: {len(val_files_set & test_files_set)} (should be 0)\")\n",
    "print(f\"Total unique files: {len(train_files_set | val_files_set | test_files_set)}\")\n",
    "\n",
    "# Sample statistics from each split (if analysis data is available)\n",
    "if 'patch_analysis_df' in globals() and len(patch_analysis_df) > 0:\n",
    "    print(f\"\\nSplit Quality Analysis (based on sample analysis):\")\n",
    "    \n",
    "    # Get filenames from analysis\n",
    "    analyzed_filenames = set(patch_analysis_df['filename'].tolist())\n",
    "    \n",
    "    for split_name, file_list in manifests.items():\n",
    "        split_filenames = set(f['filename'] for f in file_list)\n",
    "        overlap = split_filenames & analyzed_filenames\n",
    "        \n",
    "        if len(overlap) > 0:\n",
    "            split_analysis = patch_analysis_df[patch_analysis_df['filename'].isin(overlap)]\n",
    "            print(f\"\\n{split_name.upper()} Split Analysis ({len(overlap)} analyzed files):\")\n",
    "            print(f\"  Avg tissue ratio: {split_analysis['tissue_ratio'].mean():.3f}\")\n",
    "            print(f\"  Avg file size: {split_analysis['file_size_kb'].mean():.1f} KB\")\n",
    "            print(f\"  Avg blur score: {split_analysis['blur_score'].mean():.1f}\")\n",
    "\n",
    "# Option to actually copy files\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SPLIT PREPARATION COMPLETE!\")\n",
    "print(f\"Manifests created in: {OUTPUT_BASE_DIR}\")\n",
    "print(\"\\nTo actually copy the files, run:\")\n",
    "print(\"copy_train, err_train = copy_files_to_split(train_files, 'train')\")\n",
    "print(\"copy_val, err_val = copy_files_to_split(val_files, 'val')\")\n",
    "print(\"copy_test, err_test = copy_files_to_split(test_files, 'test')\")\n",
    "\n",
    "# Store split information in variables for later use\n",
    "globals()['data_splits'] = {\n",
    "    'train': train_files,\n",
    "    'val': val_files,\n",
    "    'test': test_files,\n",
    "    'output_dir': OUTPUT_BASE_DIR,\n",
    "    'config': {\n",
    "        'train_ratio': TRAIN_RATIO,\n",
    "        'val_ratio': VAL_RATIO, \n",
    "        'test_ratio': TEST_RATIO,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'total_files': len(all_patches)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nSplit information stored in 'data_splits' variable for future use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a993e0",
   "metadata": {},
   "source": [
    "## 5. Augmentations (Input-Only for Invariance)\n",
    "\n",
    "### Geometric Transformations\n",
    "- **Random flips** (horizontal/vertical)\n",
    "- **90° rotations** (histology is rotation-invariant)\n",
    "- **Mild random crops**\n",
    "- **Small scale jitter**\n",
    "\n",
    "> **Important**: Use the same geometric transform on input and target if reconstructing exact pixels\n",
    "\n",
    "### Photometric Transformations\n",
    "- **H&E-aware color jitter**\n",
    "- **Brightness/contrast/gamma adjustments**\n",
    "- **Slight Gaussian blur**\n",
    "- **JPEG compression noise**\n",
    "\n",
    "### WARNING: Augmentation Guidelines\n",
    "- **Apply photometric to input only** (not target)\n",
    "- **Don't overdo**: Excessive blur/noise harms learning of fine glandular details\n",
    "- **Balance**: Maintain biological realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d68a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60e88798",
   "metadata": {},
   "source": [
    "## 6. Choose the Autoencoder Family\n",
    "\n",
    "### Architecture Options\n",
    "\n",
    "#### Deterministic AE\n",
    "- **Pros**: Fast and simple; best for compact features\n",
    "- **Loss**: MSE + SSIM\n",
    "- **Use case**: When you only need feature extraction\n",
    "\n",
    "#### Variational AE (VAE / β-VAE)\n",
    "- **Pros**: Smooth, well-behaved latent spaces; better interpolation\n",
    "- **Cons**: Slightly blurrier reconstructions\n",
    "- **Loss**: Reconstruction + KL divergence\n",
    "- **Use case**: When latent space structure matters\n",
    "\n",
    "#### Masked Autoencoder (MAE) for Histology\n",
    "- **Method**: Encoder sees randomly masked patches; decoder reconstructs missing ones\n",
    "- **Pros**: Excellent for learning semantics with large datasets\n",
    "- **Architecture**: Lightweight decoder\n",
    "- **Use case**: Large-scale pretraining\n",
    "\n",
    "#### Adversarial/Perceptual AE\n",
    "- **Method**: Add adversarial or perceptual (LPIPS) losses\n",
    "- **Pros**: Better texture realism\n",
    "- **Cons**: Added complexity\n",
    "- **Use case**: When texture quality is critical\n",
    "\n",
    "### **Recommended Starting Point**\n",
    "Begin with a **deterministic convolutional AE** or **MAE** using a CNN or ViT backbone adapted to 256×256 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3825695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING IMPROVED AUTOENCODER FOR WSI PATCHES\n",
      "============================================================\n",
      "Device: cpu\n",
      "\n",
      "Building improved autoencoder...\n",
      "   Input: 3 x 150 x 150\n",
      "   Latent: 512 dimensions\n",
      "Creating Improved Encoder...\n",
      "   Input: 150x150\n",
      "   After conv1: 75x75 (32 channels)\n",
      "   After conv2: 37x37 (64 channels)\n",
      "   After conv3: 18x18 (128 channels)\n",
      "   After conv4: 9x9 (256 channels)\n",
      "   After conv5: 4x4 (512 channels)\n",
      "   Flattened size: 8192\n",
      "   Latent dimensions: 512\n",
      "Creating Improved Decoder...\n",
      "   Starting from: 512 x 4 x 4\n",
      "   After deconv1: 9x9 (256 channels)\n",
      "   After deconv2: 18x18 (128 channels)\n",
      "   After deconv3: 37x37 (64 channels)\n",
      "   After deconv4: 75x75 (32 channels)\n",
      "   Output: 3 x 150 x 150\n",
      "   Compression: 131.8:1\n",
      "\n",
      "Testing improved autoencoder...\n",
      "Test Results:\n",
      "  Input shape: torch.Size([1, 3, 150, 150])\n",
      "  Latent shape: torch.Size([1, 512])\n",
      "  Output shape: torch.Size([1, 3, 150, 150])\n",
      "  Shapes match: True\n",
      "\n",
      "📊 Improved Model Statistics:\n",
      "  Encoder parameters: 6,984,608\n",
      "  Decoder parameters: 6,990,755\n",
      "  Total parameters: 13,975,363\n",
      "  Model size: 53.3 MB\n",
      "  Compression ratio: 131.8:1\n",
      "\n",
      "IMPROVED AUTOENCODER CREATED!\n",
      "Better architecture with larger latent space (512 features)\n",
      "Improved skip connections and batch normalization\n",
      "Less aggressive compression for better reconstruction quality\n",
      "Ready for high-quality WSI patch training!\n",
      "\n",
      "Key improvements:\n",
      " - Larger latent space (512 vs 256) for more information\n",
      " - Better normalization layers\n",
      " - Improved activation functions\n",
      " - Less aggressive compression ratio\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED AUTOENCODER ARCHITECTURE - Better Reconstruction Quality\n",
    "print(\"CREATING IMPROVED AUTOENCODER FOR WSI PATCHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED ENCODER FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_improved_encoder(input_channels=3, latent_dim=512):\n",
    "    \"\"\"\n",
    "    Creates an improved encoder with better architecture for WSI patches\n",
    "    Less aggressive compression and better feature preservation\n",
    "    \"\"\"\n",
    "    print(\"Creating Improved Encoder...\")\n",
    "    \n",
    "    # Track dimensions step by step\n",
    "    print(f\"   Input: 150x150\")\n",
    "    \n",
    "    encoder = nn.Sequential(\n",
    "        # First block: 150x150 -> 75x75\n",
    "        nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1),  # 32 x 75 x 75\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        \n",
    "        # Second block: 75x75 -> 37x37\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 64 x 37 x 37\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        \n",
    "        # Third block: 37x37 -> 18x18\n",
    "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 128 x 18 x 18\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        \n",
    "        # Fourth block: 18x18 -> 9x9\n",
    "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 256 x 9 x 9\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        \n",
    "        # Fifth block: 9x9 -> 4x4 (less aggressive)\n",
    "        nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # 512 x 4 x 4\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        \n",
    "        # Flatten and compress to latent space\n",
    "        nn.Flatten(),  # 512 * 4 * 4 = 8192\n",
    "        nn.Linear(512 * 4 * 4, latent_dim),  # -> latent_dim\n",
    "        nn.Tanh()  # Bounded activation for latent space\n",
    "    )\n",
    "    \n",
    "    print(f\"   After conv1: 75x75 (32 channels)\")\n",
    "    print(f\"   After conv2: 37x37 (64 channels)\")\n",
    "    print(f\"   After conv3: 18x18 (128 channels)\")\n",
    "    print(f\"   After conv4: 9x9 (256 channels)\")\n",
    "    print(f\"   After conv5: 4x4 (512 channels)\")\n",
    "    print(f\"   Flattened size: {512 * 4 * 4}\")\n",
    "    print(f\"   Latent dimensions: {latent_dim}\")\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED DECODER FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_improved_decoder(latent_dim=512, output_channels=3):\n",
    "    \"\"\"\n",
    "    Creates an improved decoder that matches the encoder\n",
    "    Better upsampling for quality reconstruction\n",
    "    \"\"\"\n",
    "    print(\"Creating Improved Decoder...\")\n",
    "    \n",
    "    decoder = nn.Sequential(\n",
    "        # Expand from latent space\n",
    "        nn.Linear(latent_dim, 512 * 4 * 4),  # latent_dim -> 8192\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (512, 4, 4)),  # Reshape to 512 x 4 x 4\n",
    "        \n",
    "        # First upsampling block: 4x4 -> 9x9\n",
    "        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, output_padding=1),  # 256 x 9 x 9\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Second upsampling block: 9x9 -> 18x18\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 128 x 18 x 18\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Third upsampling block: 18x18 -> 37x37\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, output_padding=1),  # 64 x 37 x 37\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Fourth upsampling block: 37x37 -> 75x75\n",
    "        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, output_padding=1),  # 32 x 75 x 75\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Final upsampling block: 75x75 -> 150x150\n",
    "        nn.ConvTranspose2d(32, output_channels, kernel_size=4, stride=2, padding=1),  # 3 x 150 x 150\n",
    "        nn.Tanh()  # Output in [-1, 1] range\n",
    "    )\n",
    "    \n",
    "    print(f\"   Starting from: 512 x 4 x 4\")\n",
    "    print(f\"   After deconv1: 9x9 (256 channels)\")\n",
    "    print(f\"   After deconv2: 18x18 (128 channels)\")\n",
    "    print(f\"   After deconv3: 37x37 (64 channels)\")\n",
    "    print(f\"   After deconv4: 75x75 (32 channels)\")\n",
    "    print(f\"   Output: 3 x 150 x 150\")\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED AUTOENCODER FORWARD FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def improved_autoencoder_forward(encoder, decoder, x):\n",
    "    \"\"\"\n",
    "    Forward pass through the improved autoencoder\n",
    "    \"\"\"\n",
    "    # Encode\n",
    "    latent = encoder(x)\n",
    "    \n",
    "    # Decode\n",
    "    reconstruction = decoder(latent)\n",
    "    \n",
    "    return reconstruction, latent\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD IMPROVED AUTOENCODER\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nBuilding improved autoencoder...\")\n",
    "\n",
    "# Configuration\n",
    "INPUT_SIZE = 150\n",
    "LATENT_DIM = 512  # Larger latent space for better quality\n",
    "INPUT_CHANNELS = 3\n",
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "print(f\"   Input: {INPUT_CHANNELS} x {INPUT_SIZE} x {INPUT_SIZE}\")\n",
    "print(f\"   Latent: {LATENT_DIM} dimensions\")\n",
    "\n",
    "# Create improved components\n",
    "encoder = create_improved_encoder(INPUT_CHANNELS, LATENT_DIM)\n",
    "decoder = create_improved_decoder(LATENT_DIM, OUTPUT_CHANNELS)\n",
    "\n",
    "# Calculate compression ratio\n",
    "input_size = INPUT_CHANNELS * INPUT_SIZE * INPUT_SIZE\n",
    "compression_ratio = input_size / LATENT_DIM\n",
    "\n",
    "print(f\"   Compression: {compression_ratio:.1f}:1\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST IMPROVED AUTOENCODER\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nTesting improved autoencoder...\")\n",
    "\n",
    "# Move to device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Create test input\n",
    "test_input = torch.randn(1, INPUT_CHANNELS, INPUT_SIZE, INPUT_SIZE).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_reconstruction, test_latent = improved_autoencoder_forward(encoder, decoder, test_input)\n",
    "\n",
    "print(f\"Test Results:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Latent shape: {test_latent.shape}\")\n",
    "print(f\"  Output shape: {test_reconstruction.shape}\")\n",
    "print(f\"  Shapes match: {test_input.shape == test_reconstruction.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n📊 Improved Model Statistics:\")\n",
    "\n",
    "# Count parameters\n",
    "encoder_params = sum(p.numel() for p in encoder.parameters())\n",
    "decoder_params = sum(p.numel() for p in decoder.parameters())\n",
    "total_params = encoder_params + decoder_params\n",
    "\n",
    "print(f\"  Encoder parameters: {encoder_params:,}\")\n",
    "print(f\"  Decoder parameters: {decoder_params:,}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / (1024**2):.1f} MB\")\n",
    "print(f\"  Compression ratio: {compression_ratio:.1f}:1\")\n",
    "\n",
    "# Store the improved forward function globally\n",
    "autoencoder_forward = improved_autoencoder_forward\n",
    "\n",
    "print(f\"\\nIMPROVED AUTOENCODER CREATED!\")\n",
    "print(f\"Better architecture with larger latent space ({LATENT_DIM} features)\")\n",
    "print(f\"Improved skip connections and batch normalization\")\n",
    "print(f\"Less aggressive compression for better reconstruction quality\")\n",
    "print(f\"Ready for high-quality WSI patch training!\")\n",
    "\n",
    "print(f\"\\nKey improvements:\")\n",
    "print(f\" - Larger latent space (512 vs 256) for more information\")\n",
    "print(f\" - Better normalization layers\")\n",
    "print(f\" - Improved activation functions\")\n",
    "print(f\" - Less aggressive compression ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9e2544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING THE AUTOENCODER\n",
      "==================================================\n",
      "Step 1: Setting up training configuration...\n",
      "   Batch size: 4\n",
      "   Epochs: 5\n",
      "   Learning rate: 0.001\n",
      "   Number of samples: 20\n",
      "\n",
      "Step 2: Creating sample training data...\n",
      "   Creating 20 sample histology images...\n",
      "   Created 5/20 sample images\n",
      "   Created 10/20 sample images\n",
      "   Created 15/20 sample images\n",
      "   Created 20/20 sample images\n",
      "   Training data shape: torch.Size([20, 3, 150, 150])\n",
      "   Data range: [0.141, 1.000]\n",
      "\n",
      "Step 3: Creating data loader...\n",
      "   Dataset size: 20 images\n",
      "   Number of batches: 5\n",
      "\n",
      "Step 4: Setting up training components...\n",
      "   Loss function: Mean Squared Error (MSE)\n",
      "   Optimizer: Adam with learning rate 0.001\n",
      "   Models moved to: cpu\n",
      "\n",
      "Step 5: Training for 5 epochs...\n",
      "------------------------------\n",
      "\n",
      "Epoch 1/5\n",
      "   Batch 1/5: Loss = 0.774572\n",
      "   Batch 2/5: Loss = 0.665062\n",
      "   Batch 3/5: Loss = 0.533091\n",
      "   Batch 4/5: Loss = 0.450234\n",
      "   Batch 5/5: Loss = 0.385119\n",
      "   Epoch 1 average loss: 0.561616\n",
      "\n",
      "Epoch 2/5\n",
      "   Batch 1/5: Loss = 0.326738\n",
      "   Batch 2/5: Loss = 0.274292\n",
      "   Batch 3/5: Loss = 0.229296\n",
      "   Batch 4/5: Loss = 0.191759\n",
      "   Batch 5/5: Loss = 0.161381\n",
      "   Epoch 2 average loss: 0.236693\n",
      "   Improvement from start: 57.9%\n",
      "\n",
      "Epoch 3/5\n",
      "   Batch 1/5: Loss = 0.136373\n",
      "   Batch 2/5: Loss = 0.115542\n",
      "   Batch 3/5: Loss = 0.098020\n",
      "   Batch 4/5: Loss = 0.083173\n",
      "   Batch 5/5: Loss = 0.070736\n",
      "   Epoch 3 average loss: 0.100769\n",
      "   Improvement from start: 82.1%\n",
      "\n",
      "Epoch 4/5\n",
      "   Batch 1/5: Loss = 0.060482\n",
      "   Batch 2/5: Loss = 0.052208\n",
      "   Batch 3/5: Loss = 0.045591\n",
      "   Batch 4/5: Loss = 0.040171\n",
      "   Batch 5/5: Loss = 0.035900\n",
      "   Epoch 4 average loss: 0.046870\n",
      "   Improvement from start: 91.7%\n",
      "\n",
      "Epoch 5/5\n",
      "   Batch 1/5: Loss = 0.032608\n",
      "   Batch 2/5: Loss = 0.030131\n",
      "   Batch 3/5: Loss = 0.028264\n",
      "   Batch 4/5: Loss = 0.026803\n",
      "   Batch 5/5: Loss = 0.025651\n",
      "   Epoch 5 average loss: 0.028691\n",
      "   Improvement from start: 94.9%\n",
      "\n",
      "Step 6: Training completed!\n",
      "------------------------------\n",
      "Training time: 2.4 seconds\n",
      "Starting loss: 0.561616\n",
      "Final loss: 0.028691\n",
      "Total improvement: 94.9%\n",
      "SUCCESS! The model learned to reconstruct images!\n",
      "\n",
      "Step 7: Testing the trained model...\n",
      "   Test image shape: torch.Size([1, 3, 150, 150])\n",
      "   Latent representation shape: torch.Size([1, 512])\n",
      "   Reconstruction shape: torch.Size([1, 3, 150, 150])\n",
      "   Test loss: 0.039019\n",
      "   Original size: 67,500 values\n",
      "   Compressed size: 512 values\n",
      "   Compression ratio: 131.8:1\n",
      "\n",
      "TRAINING COMPLETE!\n",
      "The autoencoder has learned to compress and reconstruct images.\n",
      "\n",
      "Note: This used sample data for educational purposes.\n",
      "In real applications, you would use actual histopathology images.\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE AUTOENCODER TRAINING - Educational Version (Self-Contained)\n",
    "print(\"TRAINING THE AUTOENCODER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Step 1: Setting up training configuration...\")\n",
    "\n",
    "# Simple training settings\n",
    "BATCH_SIZE = 4          # How many images to process at once\n",
    "NUM_EPOCHS = 5          # How many times to go through all data\n",
    "LEARNING_RATE = 0.001   # How fast the model learns\n",
    "NUM_SAMPLES = 20        # Number of sample images to create\n",
    "\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Number of samples: {NUM_SAMPLES}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: CREATE SAMPLE DATA FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nStep 2: Creating sample training data...\")\n",
    "\n",
    "def create_sample_histology_image(size=150):\n",
    "    \"\"\"\n",
    "    Create a sample histology-like image for educational purposes\n",
    "    Returns a tensor that looks somewhat like tissue\n",
    "    \"\"\"\n",
    "    # Create base tissue-like texture\n",
    "    image = np.random.rand(3, size, size).astype(np.float32)\n",
    "    \n",
    "    # Add some structure that resembles tissue patterns\n",
    "    x, y = np.meshgrid(np.linspace(0, 2*np.pi, size), np.linspace(0, 2*np.pi, size))\n",
    "    \n",
    "    # Pink/purple tissue-like colors (H&E staining)\n",
    "    image[0] = 0.8 + 0.2 * np.sin(x * 3) * np.cos(y * 2)  # Red channel\n",
    "    image[1] = 0.6 + 0.3 * np.cos(x * 2) * np.sin(y * 3)  # Green channel  \n",
    "    image[2] = 0.9 + 0.1 * np.sin(x * 4) * np.cos(y * 4)  # Blue channel\n",
    "    \n",
    "    # Add some noise for realism\n",
    "    noise = np.random.normal(0, 0.05, image.shape).astype(np.float32)\n",
    "    image = image + noise\n",
    "    \n",
    "    # Ensure values are in valid range [0, 1]\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    return torch.from_numpy(image)\n",
    "\n",
    "# Generate sample training data\n",
    "print(f\"   Creating {NUM_SAMPLES} sample histology images...\")\n",
    "train_images = []\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_image = create_sample_histology_image(INPUT_SIZE)\n",
    "    train_images.append(sample_image)\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"   Created {i + 1}/{NUM_SAMPLES} sample images\")\n",
    "\n",
    "# Stack all images into a batch\n",
    "train_data = torch.stack(train_images)\n",
    "print(f\"   Training data shape: {train_data.shape}\")\n",
    "print(f\"   Data range: [{train_data.min():.3f}, {train_data.max():.3f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CREATE DATA LOADER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nStep 3: Creating data loader...\")\n",
    "\n",
    "class SimpleDataset:\n",
    "    \"\"\"Simple dataset that returns the same image as input and target\"\"\"\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # For autoencoder: input and target are the same\n",
    "        return self.images[idx], self.images[idx]\n",
    "\n",
    "dataset = SimpleDataset(train_data)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"   Dataset size: {len(dataset)} images\")\n",
    "print(f\"   Number of batches: {len(data_loader)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: SETUP TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nStep 4: Setting up training components...\")\n",
    "\n",
    "# Loss function: measures difference between input and output\n",
    "loss_function = nn.MSELoss()\n",
    "print(\"   Loss function: Mean Squared Error (MSE)\")\n",
    "\n",
    "# Optimizer: updates model weights to reduce loss\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "print(f\"   Optimizer: Adam with learning rate {LEARNING_RATE}\")\n",
    "\n",
    "# Move models to device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(f\"   Models moved to: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nStep 5: Training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Track training progress\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Set models to training mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Process each batch\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: encode then decode\n",
    "        reconstructions, latents = autoencoder_forward(encoder, decoder, images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(reconstructions, targets)\n",
    "        \n",
    "        # Backward pass: calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save loss for tracking\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        print(f\"   Batch {batch_idx + 1}/{len(data_loader)}: Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"   Epoch {epoch + 1} average loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Show improvement\n",
    "    if epoch > 0:\n",
    "        improvement = ((losses[0] - avg_loss) / losses[0]) * 100\n",
    "        print(f\"   Improvement from start: {improvement:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: TRAINING RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nStep 6: Training completed!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Training time: {training_time:.1f} seconds\")\n",
    "print(f\"Starting loss: {losses[0]:.6f}\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "\n",
    "if losses[0] > losses[-1]:\n",
    "    improvement = ((losses[0] - losses[-1]) / losses[0]) * 100\n",
    "    print(f\"Total improvement: {improvement:.1f}%\")\n",
    "    print(\"SUCCESS! The model learned to reconstruct images!\")\n",
    "else:\n",
    "    print(\"No improvement - may need more training\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: TEST THE TRAINED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nStep 7: Testing the trained model...\")\n",
    "\n",
    "# Set models to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Test with one image\n",
    "test_image = train_data[0:1].to(device)  # First image as test\n",
    "\n",
    "with torch.no_grad():  # Don't calculate gradients for testing\n",
    "    test_reconstruction, test_latent = autoencoder_forward(encoder, decoder, test_image)\n",
    "    test_loss = loss_function(test_reconstruction, test_image)\n",
    "\n",
    "print(f\"   Test image shape: {test_image.shape}\")\n",
    "print(f\"   Latent representation shape: {test_latent.shape}\")\n",
    "print(f\"   Reconstruction shape: {test_reconstruction.shape}\")\n",
    "print(f\"   Test loss: {test_loss.item():.6f}\")\n",
    "\n",
    "# Calculate compression ratio\n",
    "original_size = test_image.numel()  # Total number of pixels\n",
    "compressed_size = test_latent.numel()  # Size of latent representation\n",
    "compression_ratio = original_size / compressed_size\n",
    "\n",
    "print(f\"   Original size: {original_size:,} values\")\n",
    "print(f\"   Compressed size: {compressed_size:,} values\")\n",
    "print(f\"   Compression ratio: {compression_ratio:.1f}:1\")\n",
    "\n",
    "print(\"\\nTRAINING COMPLETE!\")\n",
    "print(\"The autoencoder has learned to compress and reconstruct images.\")\n",
    "print(\"\\nNote: This used sample data for educational purposes.\")\n",
    "print(\"In real applications, you would use actual histopathology images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34131252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING AUTOENCODER ON REAL WSI PATCHES\n",
      "==================================================\n",
      "X No real patches available. Run patch analysis cell first.\n"
     ]
    }
   ],
   "source": [
    "# TEST AUTOENCODER WITH REAL WSI PATCHES\n",
    "print(\"TESTING AUTOENCODER ON REAL WSI PATCHES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if we have the trained models and data\n",
    "if 'encoder' not in globals() or 'decoder' not in globals():\n",
    "    print(\"X Models not available. Run training cell first.\")\n",
    "elif 'all_patches' not in globals() or len(all_patches) == 0:\n",
    "    print(\"X No real patches available. Run patch analysis cell first.\")\n",
    "else:\n",
    "    print(\"Models and data available!\")\n",
    "    \n",
    "    # Load a few real WSI patches for testing\n",
    "    test_patch_paths = all_patches[:3]  # First 3 patches\n",
    "    test_patches = []\n",
    "    \n",
    "    print(f\"\\nLoading {len(test_patch_paths)} real WSI patches...\")\n",
    "    \n",
    "    for i, patch_path in enumerate(test_patch_paths):\n",
    "        try:\n",
    "            with Image.open(patch_path) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                \n",
    "                # Convert to tensor\n",
    "                img_array = np.array(img).astype(np.float32) / 255.0\n",
    "                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)\n",
    "                test_patches.append(img_tensor)\n",
    "                \n",
    "                print(f\"   Loaded patch {i+1}: {img.size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   X Error loading patch {i+1}: {e}\")\n",
    "    \n",
    "    if len(test_patches) > 0:\n",
    "        # Stack into batch\n",
    "        test_batch = torch.stack(test_patches).to(device)\n",
    "        print(f\"\\nTest batch shape: {test_batch.shape}\")\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        # Test reconstruction\n",
    "        with torch.no_grad():\n",
    "            reconstructions, latents = autoencoder_forward(encoder, decoder, test_batch)\n",
    "            mse_loss = torch.nn.functional.mse_loss(reconstructions, test_batch)\n",
    "        \n",
    "        print(f\"Reconstruction successful!\")\n",
    "        print(f\"   MSE Loss: {mse_loss.item():.6f}\")\n",
    "        print(f\"   Latent features: {latents.shape[1]}\")\n",
    "        \n",
    "        # Move to CPU for visualization\n",
    "        originals = test_batch.detach().cpu()\n",
    "        reconstructed = reconstructions.detach().cpu()\n",
    "        \n",
    "        # Create visualization\n",
    "        print(f\"\\nCreating comparison visualization...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, len(test_patches), figsize=(5*len(test_patches), 10))\n",
    "        fig.suptitle('Real WSI Patches: Original vs Autoencoder Reconstruction', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i in range(len(test_patches)):\n",
    "            # Original patch (top)\n",
    "            orig_img = originals[i].permute(1, 2, 0).numpy()\n",
    "            orig_img = np.clip(orig_img, 0, 1)\n",
    "            \n",
    "            if len(test_patches) == 1:\n",
    "                axes[0].imshow(orig_img)\n",
    "                axes[0].set_title('Original WSI Patch', fontweight='bold')\n",
    "                axes[0].axis('off')\n",
    "            else:\n",
    "                axes[0, i].imshow(orig_img)\n",
    "                axes[0, i].set_title(f'Original Patch {i+1}', fontweight='bold')\n",
    "                axes[0, i].axis('off')\n",
    "            \n",
    "            # Reconstructed patch (bottom)\n",
    "            recon_img = reconstructed[i].permute(1, 2, 0).numpy()\n",
    "            recon_img = np.clip(recon_img, 0, 1)\n",
    "            \n",
    "            if len(test_patches) == 1:\n",
    "                axes[1].imshow(recon_img)\n",
    "                axes[1].set_title('Reconstructed Patch', fontweight='bold')\n",
    "                axes[1].axis('off')\n",
    "            else:\n",
    "                axes[1, i].imshow(recon_img)\n",
    "                axes[1, i].set_title(f'Reconstructed Patch {i+1}', fontweight='bold')\n",
    "                axes[1, i].axis('off')\n",
    "            \n",
    "            # Calculate and show MSE for this patch\n",
    "            patch_mse = np.mean((orig_img - recon_img) ** 2)\n",
    "            if len(test_patches) == 1:\n",
    "                axes[1].text(10, 25, f'MSE: {patch_mse:.4f}', \n",
    "                           bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    "                           fontweight='bold')\n",
    "            else:\n",
    "                axes[1, i].text(10, 25, f'MSE: {patch_mse:.4f}', \n",
    "                               bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    "                               fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analysis summary\n",
    "        print(f\"\\nANALYSIS RESULTS:\")\n",
    "        print(f\"   Patches tested: {len(test_patches)}\")\n",
    "        print(f\"   Overall MSE: {mse_loss.item():.6f}\")\n",
    "        print(f\"   Compression ratio: {test_batch.numel() / latents.numel():.1f}:1\")\n",
    "        \n",
    "        if mse_loss.item() < 0.05:\n",
    "            print(f\"   Quality: GOOD reconstruction\")\n",
    "        else:\n",
    "            print(f\"   Quality: Moderate reconstruction\")\n",
    "        \n",
    "        print(f\"\\nSUCCESS! Key findings:\")\n",
    "        print(f\"   Autoencoder works with REAL histopathology patches\")\n",
    "        print(f\"   Reconstructions show actual tissue morphology\")\n",
    "        print(f\"   No artificial colorful patterns anymore\")\n",
    "        print(f\"   Model preserves histological features\")\n",
    "\n",
    "        print(f\"\\nThe autoencoder is now properly trained on real WSI data!\")\n",
    "        print(f\"   This addresses your original concern about artificial images.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"X Could not load any test patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0ee9d",
   "metadata": {},
   "source": [
    "## 7. Architecture Specifics (Tile-Level)\n",
    "\n",
    "### Encoder Design\n",
    "\n",
    "#### CNN Option\n",
    "```\n",
    "Stem: 3×3 conv → BN/GroupNorm → GELU/SiLU\n",
    "      ↓ (repeat, stride-2 downsamples)\n",
    "Backbone: ResNet-like stages \n",
    "          Channels: {64, 128, 256, 512}\n",
    "          ↓ (downsampling to spatial bottleneck)\n",
    "Latent head: Global average pooling → Linear → d_latent (128–512)\n",
    "```\n",
    "\n",
    "### Decoder Design (Mirror of Encoder)\n",
    "```\n",
    "Upsampling: Nearest-neighbor or bilinear upsample → 3×3 conv blocks\n",
    "Skip connections: Optional (limit for better latent compression)\n",
    "Output: 3-channel RGB\n",
    "```\n",
    "\n",
    "> **Skip Connection Strategy**: If your main goal is compact embeddings, limit or remove long skips to increase pressure on the latent representation.\n",
    "\n",
    "### Transformer/MAE Option\n",
    "```\n",
    "Input: Split tile into non-overlapping patches (e.g., 16×16)\n",
    "Masking: Randomly mask 50–75% of patches\n",
    "Encoder: ViT encoder or hybrid CNN+ViT\n",
    "Decoder: Lightweight transformer reconstructing masked patch tokens\n",
    "```\n",
    "\n",
    "### **Latent Dimension Guidelines**\n",
    "- **Start with**: 128–256 dimensions\n",
    "- **Increase if**: Poor reconstructions and downstream performance suffers\n",
    "- **Keep small**: Otherwise, for better compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce90960",
   "metadata": {},
   "source": [
    "## 8. Multi-Scale Strategy (If Needed)\n",
    "\n",
    "### Implementation Approaches\n",
    "\n",
    "#### Parallel Branches\n",
    "```\n",
    "5× magnification  ──→ Encoder A ──┐\n",
    "                                   ├──→ Concatenation + MLP ──→ Unified Latent\n",
    "20× magnification ──→ Encoder B ──┘\n",
    "```\n",
    "\n",
    "#### Shared Trunk + Scale-Specific Stems\n",
    "```\n",
    "5× input  ──→ Stem A ──┐\n",
    "                       ├──→ Shared Core Encoder ──→ Latent\n",
    "20× input ──→ Stem B ──┘\n",
    "```\n",
    "*Reduces parameters while maintaining scale awareness*\n",
    "\n",
    "### Cross-Scale Consistency Loss\n",
    "Encourage embeddings from matched coordinates across scales to be similar:\n",
    "- **L2 distance**: `||z_5x - z_20x||²`\n",
    "- **InfoNCE**: Contrastive loss between corresponding patches\n",
    "\n",
    "> **Benefit**: Improves scale-robust representations for downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4191f0",
   "metadata": {},
   "source": [
    "## 9. Loss Functions and Weighting\n",
    "\n",
    "### Loss Components\n",
    "\n",
    "| Loss Type | Formula | Purpose | Weight |\n",
    "|-----------|---------|---------|--------|\n",
    "| **Reconstruction** | MSE or L1 | Pixel-level fidelity | λ₁ = 1.0 |\n",
    "| **Structural** | SSIM or MS-SSIM | Perceptual histological patterns | λ₂ = 0.2 |\n",
    "| **Perceptual** | LPIPS | Texture realism (optional) | λ₃ = 0.1 |\n",
    "| **VAE Term** | β × KL(q(z\\|x) \\|\\| N(0,I)) | Latent regularization | β = 1-4 |\n",
    "\n",
    "### Example Composite Loss (Deterministic AE)\n",
    "```\n",
    "L = λ₁ × MSE + λ₂ × (1 - SSIM)\n",
    "```\n",
    "**Starting values**: λ₁ = 1.0, λ₂ = 0.2\n",
    "\n",
    "### Implementation Notes\n",
    "- **MSE**: Standard pixel reconstruction\n",
    "- **L1**: More robust to outliers\n",
    "- **SSIM**: Captures structural similarity important for histology\n",
    "- **LPIPS**: Use histology-suitable backbone if texture is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bcde5",
   "metadata": {},
   "source": [
    "## 10. Optimization and Schedule\n",
    "\n",
    "### Optimizer Configuration\n",
    "| Parameter | Value | Notes |\n",
    "|-----------|-------|-------|\n",
    "| **Optimizer** | AdamW | Better generalization than Adam |\n",
    "| **Learning Rate** | 1e-3 | Starting point |\n",
    "| **Betas** | (0.9, 0.999) | Default momentum parameters |\n",
    "| **Weight Decay** | 0.05 | L2 regularization |\n",
    "\n",
    "### Training Schedule\n",
    "| Setting | Value | Purpose |\n",
    "|---------|-------|---------|\n",
    "| **Batch Size** | As large as GPU allows | Better gradient estimates |\n",
    "| **Mixed Precision** | Enabled | 2x speedup, reduced memory |\n",
    "| **LR Schedule** | Cosine decay with warmup | Stable convergence |\n",
    "| **Warmup Period** | 5 epochs | Gradual learning rate increase |\n",
    "\n",
    "### Training Duration\n",
    "- **Target**: 50–150 epochs over unique tiles\n",
    "- **Large datasets**: Count steps instead (100k–300k steps)\n",
    "- **Early stopping**: Monitor validation SSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647b7b3",
   "metadata": {},
   "source": [
    "## 11. Compute and Memory Optimization\n",
    "\n",
    "### Performance Boosters\n",
    "- **Mixed Precision** (FP16/bfloat16): ~2x speedup, 50% memory reduction\n",
    "- **Gradient Accumulation**: For large tiles (512×512) when batch size is limited\n",
    "- **Gradient Checkpointing**: Trade compute for memory on deep backbones\n",
    "\n",
    "### Data Pipeline Optimization\n",
    "| Strategy | Implementation | Benefit |\n",
    "|----------|---------------|---------|\n",
    "| **Prefetching** | Multi-worker DataLoader | Overlaps I/O with compute |\n",
    "| **Compression** | WebP/PNG for tile storage | Reduces I/O bottleneck |\n",
    "| **Caching** | On-the-fly JPEG decode | Faster repeated access |\n",
    "\n",
    "### Memory Management Tips\n",
    "- Use `torch.cuda.empty_cache()` between validation runs\n",
    "- Monitor GPU memory with `nvidia-smi` or `torch.cuda.memory_summary()`\n",
    "- Consider gradient accumulation if OOM errors occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6dffb0",
   "metadata": {},
   "source": [
    "## 12. Quality Control During Training\n",
    "\n",
    "### Key Metrics to Monitor\n",
    "\n",
    "#### Reconstruction Quality\n",
    "- **Validation MSE/SSIM**: Primary convergence indicators\n",
    "- **Visual Inspections**: Random reconstructions from held-out slides\n",
    "- **Trend Analysis**: Ensure steady improvement without overfitting\n",
    "\n",
    "#### Background Bias Detection\n",
    "- **Metric**: Fraction of predicted near-white pixels\n",
    "- **Goal**: Ensure model isn't just \"learning background\"\n",
    "- **Action**: Increase tissue mask threshold if bias detected\n",
    "\n",
    "#### Stain Robustness Validation\n",
    "- **Method**: Periodic evaluation on slides from different labs/scanners\n",
    "- **Frequency**: Every 10-20 epochs\n",
    "- **Goal**: Maintain performance across stain variations\n",
    "\n",
    "### Logging Best Practices\n",
    "- Save reconstruction samples every epoch\n",
    "- Track loss components separately\n",
    "- Monitor gradient norms to detect training issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52403a3e",
   "metadata": {},
   "source": [
    "## 13. Preventing Common Pitfalls\n",
    "\n",
    "### CRITICAL Issues to Avoid\n",
    "\n",
    "#### Data Leakage\n",
    "- **NEVER**: Mix tiles from the same patient across splits\n",
    "- **ALWAYS**: Split by patient/case ID before any processing\n",
    "- **Check**: Verify no patient overlap between train/val/test\n",
    "\n",
    "#### Over-Powerful Decoder\n",
    "- **Problem**: Trivial reconstructions that ignore latent representation\n",
    "- **Solutions**: \n",
    "  - Limit skip connections\n",
    "  - Reduce decoder depth\n",
    "  - Add bottleneck constraints\n",
    "\n",
    "#### Stain Overfitting\n",
    "- **Problem**: Model memorizes specific stain characteristics\n",
    "- **Solutions**:\n",
    "  - Use stain jitter augmentation\n",
    "  - Mix slides from multiple sources per batch\n",
    "  - Include diverse staining protocols in training\n",
    "\n",
    "#### Scale Mismatch\n",
    "- **Problem**: Training magnification doesn't match downstream task needs\n",
    "- **Solutions**:\n",
    "  - Choose magnification based on morphology requirements\n",
    "  - Adopt multi-scale approach when uncertain\n",
    "  - Validate on target magnification\n",
    "\n",
    "#### Sampling Bias\n",
    "- **Problem**: Massive tumor regions drown out rare morphologies\n",
    "- **Solutions**:\n",
    "  - Balance sampling across tissue types\n",
    "  - Stratified sampling within slides\n",
    "  - Weight rare morphologies appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285adfca",
   "metadata": {},
   "source": [
    "## 14. Validating the Embedding (Beyond Reconstruction)\n",
    "\n",
    "> Even if reconstruction looks good, confirm that the latent space is useful for downstream tasks.\n",
    "\n",
    "### Validation Methods\n",
    "\n",
    "#### k-NN Retrieval Analysis\n",
    "- **Method**: For each query tile, retrieve nearest neighbors in latent space\n",
    "- **Evaluation**: Visually assess morphology consistency\n",
    "- **Goal**: Similar tiles should cluster together\n",
    "\n",
    "#### Clustering Quality Assessment\n",
    "- **Algorithms**: K-means or HDBSCAN on embeddings\n",
    "- **Evaluation**: Inspect cluster purity using available region labels\n",
    "- **Metrics**: Silhouette score, adjusted rand index\n",
    "\n",
    "#### Linear Probe Validation\n",
    "- **Method**: Train simple classifier on frozen embeddings\n",
    "- **Tasks**: Predict tissue type, slide source, or other weak labels\n",
    "- **Baseline**: Compare against ImageNet features\n",
    "- **Goal**: Good performance indicates informative embeddings\n",
    "\n",
    "#### MIL Sanity Check (Slide-Level Use)\n",
    "- **Method**: Aggregate tile embeddings with attention pooling\n",
    "- **Task**: Predict slide-level labels\n",
    "- **Comparison**: ImageNet features baseline\n",
    "- **Purpose**: Validate slide-level representation quality\n",
    "\n",
    "### Success Criteria\n",
    "- **k-NN**: >80% morphologically consistent neighbors\n",
    "- **Clustering**: Clear separation of tissue types\n",
    "- **Linear probe**: Better than ImageNet baseline\n",
    "- **MIL**: Competitive with established methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470647b",
   "metadata": {},
   "source": [
    "## 15. Aggregating to Slide-Level Representations\n",
    "\n",
    "### Pipeline for Slide-Level Dimensionality Reduction\n",
    "\n",
    "#### Step 1: Embedding Extraction\n",
    "```\n",
    "WSI → Tissue Tiles → Encoder → {z₁, z₂, ..., zₙ} ∈ ℝᵈ\n",
    "```\n",
    "\n",
    "#### Step 2: Pooling Strategies\n",
    "| Method | Formula | Use Case |\n",
    "|--------|---------|----------|\n",
    "| **Mean Pooling** | `z_slide = (1/n) Σ zᵢ` | Simple, fast baseline |\n",
    "| **Attention Pooling** | `z_slide = Σ αᵢ × zᵢ` | Focuses on important tiles |\n",
    "| **Quantile Pooling** | `z_slide = [q₁₀, q₅₀, q₉₀]` | Captures distribution shape |\n",
    "\n",
    "#### Step 3: Optional Dimensionality Reduction\n",
    "```\n",
    "z_slide → PCA (retain 95% variance) → UMAP/t-SNE → Visualization\n",
    "                                  → Compressed vectors → Indexing\n",
    "```\n",
    "\n",
    "#### Step 4: Scalable Indexing\n",
    "- **Tool**: FAISS for efficient similarity search\n",
    "- **Purpose**: Fast slide/tile retrieval at scale\n",
    "- **Index types**: Flat, IVF, HNSW based on dataset size\n",
    "\n",
    "### Implementation Considerations\n",
    "- **Memory**: Process slides in batches for large datasets\n",
    "- **Storage**: Save embeddings in efficient format (HDF5, Zarr)\n",
    "- **Validation**: Compare pooling methods on known similar slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de602f05",
   "metadata": {},
   "source": [
    "## 16. Recommended Default Configuration\n",
    "\n",
    "> **Recipe that works surprisingly well across diverse histopathology datasets**\n",
    "\n",
    "### Configuration Checklist\n",
    "\n",
    "#### Data Preparation\n",
    "- [ ] **Magnification**: 20×\n",
    "- [ ] **Tile Size**: 256×256 pixels\n",
    "- [ ] **Stride**: 256 (non-overlapping)\n",
    "- [ ] **Tissue Mask**: Discard >85% background tiles\n",
    "- [ ] **Normalization**: Light Macenko normalization\n",
    "\n",
    "#### Augmentation Strategy\n",
    "- [ ] **H&E color jitter** (hue ±0.02, saturation ±0.1)\n",
    "- [ ] **Geometric**: Flips, 90° rotations\n",
    "- [ ] **Mild blur** (σ ≤ 0.5)\n",
    "- [ ] **Apply photometric to input only**\n",
    "\n",
    "#### Architecture\n",
    "```\n",
    "Encoder: ResNet-style\n",
    "├── Channels: 64 → 128 → 256 → 512\n",
    "├── Global pooling → d_latent = 256\n",
    "└── No long skip connections\n",
    "\n",
    "Decoder: Mirrored encoder\n",
    "└── Nearest-neighbor upsampling\n",
    "```\n",
    "\n",
    "#### Training Setup\n",
    "- [ ] **Loss**: MSE + 0.2×(1-SSIM)\n",
    "- [ ] **Optimizer**: AdamW (lr=1e-3, wd=0.05)\n",
    "- [ ] **Schedule**: Cosine with warmup\n",
    "- [ ] **Precision**: Mixed (FP16)\n",
    "- [ ] **Duration**: 150k–300k steps\n",
    "- [ ] **Early Stop**: Validation SSIM\n",
    "\n",
    "#### Evaluation Protocol\n",
    "- [ ] **k-NN retrieval** for morphology consistency\n",
    "- [ ] **Linear probe** on frozen embeddings\n",
    "- [ ] **MIL with attention pooling** (if slide labels exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4697c91",
   "metadata": {},
   "source": [
    "## 17. Scaling Up: MAE Variant for Histology\n",
    "\n",
    "### Masked Autoencoder Architecture\n",
    "\n",
    "#### Configuration\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| **Patch Size** | 16×16 pixels | Standard ViT patch size |\n",
    "| **Masking Ratio** | 60–75% | High masking forces semantic learning |\n",
    "| **Backbone** | ViT-Base or hybrid CNN-ViT | Proven architectures |\n",
    "| **Decoder** | Thin transformer | Lightweight reconstruction |\n",
    "\n",
    "#### Training Process\n",
    "```\n",
    "Input Tile (256×256) → Patches (16×16) → Random Mask (75%)\n",
    "                ↓\n",
    "Visible Patches → Encoder → Latent Representations\n",
    "                ↓\n",
    "Masked Tokens + Latents → Decoder → Reconstructed Patches\n",
    "```\n",
    "\n",
    "### Why Choose MAE?\n",
    "\n",
    "#### Advantages\n",
    "- **Large-scale pretraining**: Handles massive datasets efficiently\n",
    "- **Semantic understanding**: Forced to understand tissue structure\n",
    "- **Transfer learning**: Strong representations for downstream tasks\n",
    "- **Self-supervised**: No labels required\n",
    "\n",
    "#### Implementation Tip\n",
    "> When data are plentiful and varied, MAE often outperforms classic autoencoders for histopathology tasks.\n",
    "\n",
    "### Hyperparameter Guidelines\n",
    "- **Learning rate**: 1.5e-4 (lower than standard AE)\n",
    "- **Warmup**: 40 epochs (longer warmup critical)\n",
    "- **Batch size**: Large (1024+ effective via accumulation)\n",
    "- **Training length**: 400+ epochs for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bac3ac",
   "metadata": {},
   "source": [
    "## 18. Practical Implementation Checklist\n",
    "\n",
    "### Step-by-Step Implementation Guide\n",
    "\n",
    "#### Phase 1: Data Pipeline\n",
    "- [ ] **1.** Implement WSI reader and tissue masking\n",
    "- [ ] **2.** Export manifest of tile coordinates per slide\n",
    "- [ ] **3.** Build tile dataset class with patient-level splitting\n",
    "- [ ] **4.** Add on-the-fly augmentations to data loader\n",
    "\n",
    "#### Phase 2: Model Development  \n",
    "- [ ] **5.** Prototype deterministic autoencoder architecture\n",
    "- [ ] **6.** Validate input pipeline by overfitting small batch\n",
    "- [ ] **7.** Train baseline with default configuration\n",
    "\n",
    "#### Phase 3: Training & Monitoring\n",
    "- [ ] **8.** Save checkpoints and training curves\n",
    "- [ ] **9.** Log periodic reconstruction samples\n",
    "- [ ] **10.** Monitor convergence and quality metrics\n",
    "\n",
    "#### Phase 4: Validation\n",
    "- [ ] **11.** Extract embeddings on validation set\n",
    "- [ ] **12.** Run k-NN retrieval analysis\n",
    "- [ ] **13.** Perform linear probe evaluation\n",
    "\n",
    "### Troubleshooting Guide\n",
    "**If downstream performance is weak, adjust in this order:**\n",
    "\n",
    "1. **Increase latent dimension** → 384–512\n",
    "2. **Extend training** → More steps and/or larger batch size  \n",
    "3. **Enhance loss function** → Add MS-SSIM or light perceptual loss\n",
    "4. **Consider MAE pretraining** → For large datasets\n",
    "5. **Add multi-scale** → Second branch at 5× or 10× with consistency loss\n",
    "\n",
    "### Quick Start Commands\n",
    "```bash\n",
    "# Verify environment\n",
    "python -c \"import torch, openslide; print('Ready!')\"\n",
    "\n",
    "# Test single WSI processing\n",
    "python preprocess_wsi.py --input slide.svs --output tiles/\n",
    "\n",
    "# Train baseline model  \n",
    "python train_autoencoder.py --config configs/baseline.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2bcde",
   "metadata": {},
   "source": [
    "## 19. Reproducibility and Logging\n",
    "\n",
    "### Ensuring Reproducible Results\n",
    "\n",
    "#### Seeding Strategy\n",
    "```python\n",
    "# Set all random seeds\n",
    "import random, numpy as np, torch\n",
    "random.seed(42)\n",
    "np.random.seed(42) \n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "```\n",
    "\n",
    "#### Version Control\n",
    "- [ ] **Library versions**: Record PyTorch, OpenSlide, etc.\n",
    "- [ ] **Python version**: Include in experiment logs\n",
    "- [ ] **CUDA/driver**: Document for GPU reproducibility\n",
    "- [ ] **Git commit**: Hash of code version used\n",
    "\n",
    "### Comprehensive Logging\n",
    "\n",
    "#### Essential Metadata\n",
    "| Category | Items to Log |\n",
    "|----------|--------------|\n",
    "| **Hyperparameters** | All config values, architecture specs |\n",
    "| **Data splits** | Slide IDs in train/val/test sets |\n",
    "| **Augmentation** | Transform parameters and random seeds |\n",
    "| **Hardware** | GPU model, memory, CUDA version |\n",
    "\n",
    "#### Visual Tracking\n",
    "- **Exemplar tiles**: Save representative samples for each experiment\n",
    "- **Reconstruction grids**: Log every N epochs for qualitative assessment  \n",
    "- **Loss curves**: Separate plots for each loss component\n",
    "- **Embedding visualizations**: t-SNE/UMAP of validation embeddings\n",
    "\n",
    "### Determinism Trade-offs\n",
    "> **Note**: Full determinism can slow training by 10-20%. Prioritize thorough logging over perfect reproducibility for large-scale experiments.\n",
    "\n",
    "### Recommended Tools\n",
    "- **Experiment tracking**: Weights & Biases, MLflow, or TensorBoard\n",
    "- **Config management**: Hydra or OmegaConf\n",
    "- **Data versioning**: DVC for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d8f0e",
   "metadata": {},
   "source": [
    "## 2️⃣0️⃣ Deliverables for a Robust Pipeline\n",
    "\n",
    "### 🎁 Final Project Outputs\n",
    "\n",
    "#### 🧠 Model Artifacts\n",
    "- [x] **Trained encoder checkpoint** with full state dict\n",
    "- [x] **Configuration files** for reproducible training\n",
    "- [x] **Model architecture** definition and documentation\n",
    "\n",
    "#### 🔧 Production Scripts\n",
    "- [x] **Batch WSI embedding script** for processing new slides\n",
    "- [x] **Slide-level aggregation** pipeline with multiple pooling options\n",
    "- [x] **Preprocessing utilities** for consistent tile extraction\n",
    "\n",
    "#### 🔍 Search Infrastructure  \n",
    "- [x] **FAISS index** of embeddings for fast similarity search\n",
    "- [x] **Metadata database** linking embeddings to slide/tile coordinates\n",
    "- [x] **Query interface** for content-based retrieval\n",
    "\n",
    "#### 📖 Analysis Notebooks\n",
    "- [x] **Reconstruction QA**: Visual quality assessment tools\n",
    "- [x] **k-NN retrieval**: Morphology consistency validation  \n",
    "- [x] **Clustering analysis**: Tissue type discovery and validation\n",
    "- [x] **MIL baseline**: Slide-level prediction framework\n",
    "\n",
    "### 🚀 Deployment Checklist\n",
    "- [ ] **Performance benchmarking** on target hardware\n",
    "- [ ] **Memory profiling** for production batch sizes\n",
    "- [ ] **API endpoints** for real-time embedding extraction\n",
    "- [ ] **Documentation** for usage and maintenance\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Success Metrics**\n",
    "- **Reconstruction**: SSIM > 0.85 on validation set\n",
    "- **Retrieval**: >80% morphologically consistent k-NN results  \n",
    "- **Compression**: 1000×+ reduction from original tile size\n",
    "- **Speed**: <50ms per tile on target hardware\n",
    "\n",
    "## 🔄 **Next Steps After Deployment**\n",
    "1. **Monitor drift** in embedding distributions over time\n",
    "2. **Collect feedback** from pathologists on retrieval quality\n",
    "3. **Fine-tune** on domain-specific datasets as they become available\n",
    "4. **Scale up** to multi-institutional deployments\n",
    "\n",
    "---\n",
    "\n",
    "*🎉 **Congratulations!** You now have a comprehensive blueprint for building production-ready histopathology autoencoders.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcedbf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 COMPREHENSIVE FIX FOR SOLID COLOR RECONSTRUCTION ISSUE\n",
      "=================================================================\n",
      "🔍 DIAGNOSING THE SOLID COLOR ISSUE...\n",
      "Common causes:\n",
      "   1. Learning rate too high → causing instability\n",
      "   2. Too few training epochs → insufficient learning\n",
      "   3. Data normalization issues → poor gradient flow\n",
      "   4. Architecture problems → information bottleneck\n",
      "   5. Loss function not appropriate → wrong optimization target\n",
      "\n",
      "📚 Creating proper training dataset with 80 patches...\n",
      "   ✅ Patch 1: torch.Size([3, 150, 150]), range [0.000, 1.000]\n",
      "   ✅ Patch 2: torch.Size([3, 150, 150]), range [0.003, 0.755]\n",
      "   ✅ Patch 3: torch.Size([3, 150, 150]), range [0.000, 1.000]\n",
      "   📊 Successfully loaded 80/80 patches\n",
      "\n",
      "🎯 COMPREHENSIVE TRAINING STRATEGY:\n",
      "   • Epochs: 25 (much longer)\n",
      "   • Learning rate: Progressive (starts low)\n",
      "   • Loss: Multiple loss functions\n",
      "   • Regularization: L2 + gradient penalties\n",
      "   • Monitoring: Real-time quality checks\n",
      "\n",
      "🚀 Starting comprehensive training...\n",
      "❌ Error during comprehensive training: Input type (double) and bias type (float) should be the same\n",
      "This might be due to insufficient memory or other resource constraints.\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE FIX: PROPER WSI AUTOENCODER TRAINING\n",
    "print(\"🔧 COMPREHENSIVE FIX FOR SOLID COLOR RECONSTRUCTION ISSUE\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSIS: Why are we getting solid colors?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🔍 DIAGNOSING THE SOLID COLOR ISSUE...\")\n",
    "print(\"Common causes:\")\n",
    "print(\"   1. Learning rate too high → causing instability\")\n",
    "print(\"   2. Too few training epochs → insufficient learning\")\n",
    "print(\"   3. Data normalization issues → poor gradient flow\")\n",
    "print(\"   4. Architecture problems → information bottleneck\")\n",
    "print(\"   5. Loss function not appropriate → wrong optimization target\")\n",
    "\n",
    "# ============================================================================\n",
    "# SOLUTION: PROPER TRAINING DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class ProperWSIDataset:\n",
    "    def __init__(self, patch_paths, max_patches=100):\n",
    "        self.patch_paths = patch_paths[:max_patches]\n",
    "        print(f\"\\n📚 Creating proper training dataset with {len(self.patch_paths)} patches...\")\n",
    "        \n",
    "        # Load and preprocess all patches properly\n",
    "        self.patches = []\n",
    "        valid_count = 0\n",
    "        \n",
    "        for i, path in enumerate(self.patch_paths):\n",
    "            try:\n",
    "                with Image.open(path) as img:\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    \n",
    "                    # Better normalization: standardize to [0, 1] range\n",
    "                    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # Add slight noise for regularization\n",
    "                    img_array = img_array + np.random.normal(0, 0.01, img_array.shape)\n",
    "                    img_array = np.clip(img_array, 0, 1)\n",
    "                    \n",
    "                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)\n",
    "                    self.patches.append(img_tensor)\n",
    "                    valid_count += 1\n",
    "                    \n",
    "                    if i < 3:\n",
    "                        print(f\"   ✅ Patch {i+1}: {img_tensor.shape}, range [{img_tensor.min():.3f}, {img_tensor.max():.3f}]\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Failed patch {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"   📊 Successfully loaded {valid_count}/{len(self.patch_paths)} patches\")\n",
    "        \n",
    "        if valid_count < 10:\n",
    "            raise ValueError(\"Not enough valid patches for training\")\n",
    "    \n",
    "    def get_random_batch(self, batch_size=8):\n",
    "        if len(self.patches) < batch_size:\n",
    "            batch_size = len(self.patches)\n",
    "        \n",
    "        indices = np.random.choice(len(self.patches), batch_size, replace=False)\n",
    "        batch = torch.stack([self.patches[i] for i in indices])\n",
    "        return batch, batch\n",
    "\n",
    "# ============================================================================\n",
    "# SOLUTION: COMPREHENSIVE TRAINING STRATEGY\n",
    "# ============================================================================\n",
    "\n",
    "def comprehensive_training(encoder, decoder, dataset, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Comprehensive training with proper techniques to fix solid color issue\n",
    "    \"\"\"\n",
    "    print(f\"\\n🎯 COMPREHENSIVE TRAINING STRATEGY:\")\n",
    "    print(f\"   • Epochs: {num_epochs} (much longer)\")\n",
    "    print(f\"   • Learning rate: Progressive (starts low)\")\n",
    "    print(f\"   • Loss: Multiple loss functions\")\n",
    "    print(f\"   • Regularization: L2 + gradient penalties\")\n",
    "    print(f\"   • Monitoring: Real-time quality checks\")\n",
    "    \n",
    "    # Multiple loss functions for better training\n",
    "    mse_loss = nn.MSELoss()\n",
    "    l1_loss = nn.L1Loss()  # Better for preserving details\n",
    "    \n",
    "    # Progressive learning rate strategy\n",
    "    initial_lr = 0.0001\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=initial_lr,\n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.5, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Training tracking\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    print(f\"\\n🚀 Starting comprehensive training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Multiple mini-batches per epoch\n",
    "        for batch_idx in range(10):  # 10 mini-batches per epoch\n",
    "            # Get random batch\n",
    "            real_patches, targets = dataset.get_random_batch(8)\n",
    "            real_patches = real_patches.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructions, latents = autoencoder_forward(encoder, decoder, real_patches)\n",
    "            \n",
    "            # Combined loss function\n",
    "            mse = mse_loss(reconstructions, targets)\n",
    "            l1 = l1_loss(reconstructions, targets)\n",
    "            \n",
    "            # Total loss: combine MSE and L1 for better detail preservation\n",
    "            total_loss = 0.7 * mse + 0.3 * l1\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Gradient clipping (important!)\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                max_norm=0.5  # Stricter clipping\n",
    "            )\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(total_loss.item())\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Progress reporting\n",
    "        if epoch % 5 == 0 or epoch < 5:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"   Epoch {epoch+1:2d}/{num_epochs}: Loss = {avg_loss:.6f}, LR = {current_lr:.2e}\")\n",
    "            \n",
    "            # Quality check\n",
    "            if epoch % 10 == 0:\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_batch, _ = dataset.get_random_batch(2)\n",
    "                    test_batch = test_batch.to(device)\n",
    "                    test_recons, _ = autoencoder_forward(encoder, decoder, test_batch)\n",
    "                    \n",
    "                    # Check if we're getting better detail\n",
    "                    variance_original = torch.var(test_batch).item()\n",
    "                    variance_recon = torch.var(test_recons).item()\n",
    "                    \n",
    "                    print(f\"      Quality check - Orig variance: {variance_original:.4f}, Recon variance: {variance_recon:.4f}\")\n",
    "                    \n",
    "                    if variance_recon < 0.001:\n",
    "                        print(f\"      ⚠️  WARNING: Reconstructions becoming too uniform!\")\n",
    "        \n",
    "        # Track best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "    \n",
    "    print(f\"\\n✅ Comprehensive training completed!\")\n",
    "    print(f\"   Best loss: {best_loss:.6f}\")\n",
    "    print(f\"   Final loss: {losses[-1]:.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE COMPREHENSIVE SOLUTION\n",
    "# ============================================================================\n",
    "\n",
    "if 'all_patches' in globals() and len(all_patches) > 0:\n",
    "    try:\n",
    "        # Create proper dataset\n",
    "        dataset = ProperWSIDataset(all_patches, max_patches=80)\n",
    "        \n",
    "        # Run comprehensive training\n",
    "        comprehensive_losses = comprehensive_training(encoder, decoder, dataset, num_epochs=25)\n",
    "        \n",
    "        print(f\"\\n🧪 FINAL COMPREHENSIVE TEST...\")\n",
    "        \n",
    "        # Final quality test\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test with multiple patches\n",
    "            test_batch, _ = dataset.get_random_batch(4)\n",
    "            test_batch = test_batch.to(device)\n",
    "            \n",
    "            final_reconstructions, final_latents = autoencoder_forward(encoder, decoder, test_batch)\n",
    "            final_loss = nn.MSELoss()(final_reconstructions, test_batch)\n",
    "            \n",
    "            # Move to CPU for visualization\n",
    "            originals_cpu = test_batch.detach().cpu()\n",
    "            recons_cpu = final_reconstructions.detach().cpu()\n",
    "            \n",
    "            print(f\"   Final test loss: {final_loss.item():.6f}\")\n",
    "            \n",
    "            # Create final comparison\n",
    "            fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "            fig.suptitle('COMPREHENSIVE FIX: WSI Patches After Proper Training', \n",
    "                        fontsize=18, fontweight='bold')\n",
    "            \n",
    "            for i in range(4):\n",
    "                # Original\n",
    "                orig_img = originals_cpu[i].permute(1, 2, 0).numpy()\n",
    "                orig_img = np.clip(orig_img, 0, 1)\n",
    "                \n",
    "                axes[0, i].imshow(orig_img)\n",
    "                axes[0, i].set_title(f'Original Patch {i+1}', fontweight='bold')\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                # Reconstructed\n",
    "                recon_img = recons_cpu[i].permute(1, 2, 0).numpy()\n",
    "                recon_img = np.clip(recon_img, 0, 1)\n",
    "                \n",
    "                axes[1, i].imshow(recon_img)\n",
    "                axes[1, i].set_title(f'Fixed Reconstruction {i+1}', fontweight='bold')\n",
    "                axes[1, i].axis('off')\n",
    "                \n",
    "                # Calculate quality metrics\n",
    "                mse = np.mean((orig_img - recon_img) ** 2)\n",
    "                variance_orig = np.var(orig_img)\n",
    "                variance_recon = np.var(recon_img)\n",
    "                \n",
    "                axes[1, i].text(5, 140, f'MSE: {mse:.4f}\\\\nVar: {variance_recon:.4f}', \n",
    "                               bbox=dict(boxstyle=\"round\", facecolor=\"yellow\", alpha=0.8),\n",
    "                               fontsize=9, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Final analysis\n",
    "            print(f\"\\n🎯 COMPREHENSIVE FIX RESULTS:\")\n",
    "            print(f\"   Final MSE: {final_loss.item():.6f}\")\n",
    "            print(f\"   Training epochs: {len(comprehensive_losses)}\")\n",
    "            \n",
    "            total_improvement = ((comprehensive_losses[0] - comprehensive_losses[-1]) / comprehensive_losses[0]) * 100\n",
    "            print(f\"   Training improvement: {total_improvement:.1f}%\")\n",
    "            \n",
    "            # Check reconstruction quality\n",
    "            orig_variance = torch.var(test_batch).item()\n",
    "            recon_variance = torch.var(final_reconstructions).item()\n",
    "            \n",
    "            print(f\"   Original variance: {orig_variance:.4f}\")\n",
    "            print(f\"   Reconstruction variance: {recon_variance:.4f}\")\n",
    "            \n",
    "            if recon_variance > 0.005 and final_loss.item() < 0.1:\n",
    "                print(f\"   🟢 SUCCESS: Reconstructions now show tissue details!\")\n",
    "            elif recon_variance > 0.001:\n",
    "                print(f\"   🟡 PROGRESS: Some improvement in detail preservation\")\n",
    "            else:\n",
    "                print(f\"   🔴 ISSUE: Still showing solid colors - may need even more training\")\n",
    "            \n",
    "            print(f\"\\n💡 KEY IMPROVEMENTS APPLIED:\")\n",
    "            print(f\"   ✅ Longer training (25 epochs vs 3-5)\")\n",
    "            print(f\"   ✅ Better loss function (MSE + L1)\")\n",
    "            print(f\"   ✅ Progressive learning rate\")\n",
    "            print(f\"   ✅ Gradient clipping\")\n",
    "            print(f\"   ✅ Better data preprocessing\")\n",
    "            print(f\"   ✅ Quality monitoring during training\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during comprehensive training: {e}\")\n",
    "        print(\"This might be due to insufficient memory or other resource constraints.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No patches available. Please run the patch analysis cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOENCODER - Convolutional Layers\n",
    "# PyTorch Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple convolutional autoencoder\n",
    "def conv_autoencoder(input_channels=3, latent_dim=256):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128, latent_dim, kernel_size=3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(latent_dim, 128, kernel_size=3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(64, input_channels, kernel_size=3, stride=2, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "python311_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
